# Revelio Finance âœ¨

**Revelio Finance** est une application web de finances personnelles conÃ§ue pour apporter de la clartÃ© Ã  vos dÃ©penses grÃ¢ce Ã  une analyse locale, privÃ©e et intelligente.

Le nom est un clin d'Å“il au sortilÃ¨ge de RÃ©vÃ©lation (*Revelio*) de l'univers Harry Potter. L'objectif de l'application est de "rÃ©vÃ©ler" ce qui se cache dans vos relevÃ©s bancaires.

## ğŸ¯ Le Concept

- **ConfidentialitÃ© d'abord** : L'application fonctionne entiÃ¨rement en local. Vous importez vos transactions via des fichiers OFX. Aucune donnÃ©e financiÃ¨re ne quitte votre machine.
- **Analyse Intelligente** : L'application intÃ¨gre une intelligence artificielle (LLM) locale pour offrir une catÃ©gorisation sÃ©mantique, extrayant le marchand, la catÃ©gorie et la ville de chaque transaction.

## âš™ï¸ Statut Actuel (Phase 2 : L'Intelligence)

Le projet a atteint sa deuxiÃ¨me phase. Il permet de :

- **Importer** un fichier de transactions au format OFX via une interface web.
- **Analyser** le fichier pour en extraire les transactions.
- **Enrichir** chaque transaction en utilisant un LLM local (via Ollama) pour identifier le marchand, la catÃ©gorie, et la ville.
- **Afficher** les rÃ©sultats dans un tableau clair et lisible.

## ğŸ“‚ Structure du Projet

```
revelio-finance/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py             # Le serveur API FastAPI
â”‚   â”œâ”€â”€ ai_service.py       # Le service d'interaction avec Ollama
â”‚   â”œâ”€â”€ ofx_parser.py       # Le parser pour les fichiers OFX
â”‚   â”œâ”€â”€ routers/            # Les routeurs de l'API
â”‚   â””â”€â”€ tests/              # Les tests unitaires
â””â”€â”€ frontend/
    â””â”€â”€ index.html          # L'interface utilisateur web
```

## ğŸš€ DÃ©marrage Rapide

Pour faire fonctionner l'application, vous devez lancer deux services : le **serveur d'IA (Ollama)** et le **serveur de l'application (FastAPI)**.

### 1. PrÃ©requis

- Python 3.8+
- [Ollama](https://ollama.com) installÃ© sur votre machine.

### 2. DÃ©marrage du Serveur d'IA

1.  **TÃ©lÃ©chargez le modÃ¨le LLM :** Ouvrez un terminal et exÃ©cutez la commande suivante.
    ```bash
    ollama pull llama3.2:3b
    ```

2.  **Assurez-vous qu'Ollama est en cours d'exÃ©cution.** L'application Ollama doit tourner en arriÃ¨re-plan pour que le backend puisse s'y connecter.

### 3. DÃ©marrage du Serveur de l'Application

1.  **Clonez le projet** (si ce n'est pas dÃ©jÃ  fait) et naviguez dans le dossier.

2.  **CrÃ©ez un environnement virtuel** (recommandÃ©) :
    ```bash
    python -m venv venv
    source venv/bin/activate  # Sur macOS/Linux
    .\venv\Scripts\activate   # Sur Windows
    ```

3.  **Installez les dÃ©pendances** Python :
    ```bash
    pip install -r requirements.txt
    ```

4.  **Lancez le serveur FastAPI :** Depuis la racine du projet, exÃ©cutez :
    ```bash
    uvicorn backend.main:app --reload
    ```

### 4. Utilisation

1.  Une fois le serveur dÃ©marrÃ©, ouvrez votre navigateur web et allez Ã  l'adresse :
    `http://localhost:8000`

2.  Utilisez l'interface pour sÃ©lectionner votre fichier `.ofx` ou `.qfx` et cliquez sur "Analyser".

3.  Les transactions, enrichies par l'IA, apparaÃ®tront dans un tableau sur la page.

## ğŸ› ï¸ Stack Technique

- **Backend** : Python, FastAPI, Uvicorn
- **Analyse de donnÃ©es** : ofxtools
- **Intelligence Artificielle** : Ollama
- **Frontend** : HTML, JavaScript (utilisant l'API Fetch)
- **Tests** : Pytest, pytest-asyncio, Respx